{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_analysis.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyONd1ehnggkXI3mkRjzg5hD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AslanDevbrat/NLP/blob/main/sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Processing"
      ],
      "metadata": {
        "id": "n_D1XjVpXecR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "J872ZMyQXeEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fG8QoDWA_2uA",
        "outputId": "64a62c48-4ac0-4c32-bab7-4b2cbcc31dd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'deep-learning-with-tensorflow-2' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Neuralearn/deep-learning-with-tensorflow-2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6buLmvHGPr6",
        "outputId": "bbe23979-1052-4977-d0ed-88e37b204d20"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-17 11:34:04--  https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz.1’\n",
            "\n",
            "aclImdb_v1.tar.gz.1 100%[===================>]  80.23M  19.3MB/s    in 8.5s    \n",
            "\n",
            "2022-07-17 11:34:13 (9.43 MB/s) - ‘aclImdb_v1.tar.gz.1’ saved [84125825/84125825]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzf /content/aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "id": "Bc4In9jsGXm2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from tensorflow.keras.preprocessing import text_dataset_from_directory\n",
        "from tensorflow.keras.layers import SimpleRNN,LSTM\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40gSjv4bADoL",
        "outputId": "830d0875-1992-4814-be80-e8a7e104ec1f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_directory = \"/content/aclImdb/train\"\n",
        "val_directory = \"/content/aclImdb/test\""
      ],
      "metadata": {
        "id": "OMonVZ_PHB2Y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/aclImdb/train/unsup /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbwztOYsJmsc",
        "outputId": "f5e2ad48-c33e-431e-e9d9-95a5d7725a9d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat '/content/aclImdb/train/unsup': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = text_dataset_from_directory(\n",
        "    train_directory,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiT2JT-GHRtd",
        "outputId": "057b3b06-4acd-49f1-d8c8-37ad5632ecc8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = text_dataset_from_directory(\n",
        "    val_directory,\n",
        "    shuffle = True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDmWPUVKHm6i",
        "outputId": "b7ec6f61-90bf-40d6-94f9-f9b0ff4eba90"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in train_dataset.take(1):\n",
        "  print(x)\n",
        "  print()\n",
        "  print(y)"
      ],
      "metadata": {
        "id": "XTijiiqdJ1zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentences(input_data):\n",
        "    '''\n",
        "    Input: raw reviews\n",
        "    output: standardized reviews\n",
        "    '''\n",
        "    output=tf.strings.lower(input_data)\n",
        "    outputs=tf.strings.regex_replace(output,\"<[^>]+>\",\"\")\n",
        "    outputs=tf.strings.regex_replace(output,\"<[%s]\"%re.escape(string.punctuation),\" \")\n",
        "    outputs=tf.strings.regex_replace(output,\"  \",\" \")\n",
        "    \n",
        "    return output"
      ],
      "metadata": {
        "id": "KMIBL9qDVZ5y"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentences(input_data):\n",
        "\n",
        "     output = tf.strings.lower(input_data)\n",
        "     output = tf.strings.regex_replace(output, \"<[^>]+>\",\"\")\n",
        "     output = tf.strings.regex_replace(output,'[%s]' %re.escape(string.punctuation), \"  \")\n",
        "     output = tf.strings.regex_replace(output, '  ',\" \"  )\n",
        "     lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "     output = tf.strings.join([lemmatizer.lemmatize(str(word)[12:-26], pos = \"a\") for word in tf.strings.split(output)], separator = \" \")\n",
        "     return output\n",
        "preprocess_sentences(\"I kind of consider myself as the #1 fan of Hidden Frontier\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbKsA4WYKC0N",
        "outputId": "dbfc84de-adee-4f8d-ce57-69528fcad562"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=string, numpy=b'i kind of consider myself as the 1 fan of hidden frontier'>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE=2000\n",
        "SEQUENCE_LENGTH=100\n",
        "vectorize_layer=TextVectorization(\n",
        "    standardize=preprocess_sentences,\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=SEQUENCE_LENGTH,\n",
        ")\n",
        "training_data=train_dataset.map(lambda x,y:x)### inputsxandyand outputsx\n",
        "vectorize_layer.adapt(training_data)### Adapt the vectorize Layer to the training data"
      ],
      "metadata": {
        "id": "1_UHiFcc9A8K"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorizer(review, label):\n",
        "  return tf.one_hot(vectorize_layer(review), depth = VOCAB_SIZE), label\n"
      ],
      "metadata": {
        "id": "LeyD0T3p3seD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.map(vectorizer)\n",
        "val_dataset = val_dataset.map(vectorizer)"
      ],
      "metadata": {
        "id": "zB7o5OOz4ePe"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x , y in train_dataset.take(1):\n",
        "  print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLHlkkDM4yOs",
        "outputId": "091b2849-ce80-48c9-ab86-0d404c28e636"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 1 1 1 0 0 1 1 0 0 1 1 0], shape=(32,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jjbwN2KIW57u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}